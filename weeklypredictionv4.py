# -*- coding: utf-8 -*-
"""weeklyPredictionV4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gamGBhCk78UwMWaOzcFnislYsU9h95WA
"""

# lib imports 
import matplotlib.pyplot as plt
from scipy.ndimage.interpolation import shift
from IPython.display import SVG


from keras.utils.vis_utils import model_to_dot
import numpy as np
import gc
import time
import pandas as pd
import keras 
import os
import pydot
import graphviz

from sklearn.metrics import explained_variance_score
from sklearn.metrics import accuracy_score
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Activation, Dense
from keras.layers import LSTM
import tensorflow as tf
from keras.layers import Dropout
from sklearn.metrics import mean_squared_error

import datetime
import IPython


# combination of possible feature vectors 
columnTOProcess = []
Versio1 = ['price_close','volume_trade','trade_count']
Versio2 = ['price_close','volume_trade']
Versio3 = ['price_close','trade_count']
Versio4 = ['price_close']

# error rate detection in the actual and predicted data
def mean_absolute_percentage_error_custom(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    abs_Data = np.abs((y_true - y_pred) / y_true)
    return np.mean(abs_Data) * 100
  
# convert the normal array into numpy array for processing data 
def to_array(data):
  x = [np.array(data[i]) for i in range (len(data))]
  return np.array(x) 

# data split for testing and training set 
def split_data_for_LSTM(data, training_size=0.8):
  global window
  return data[:int(len(data))], data[(int(len(data) - window - 3)):]

# windows of sequence for the LSTM
def data_input_for_LSTM(data, wnd=window):
  global columnTOProcess
  norm_cols = columnTOProcess
  LSTM_inputs = []
  for i in range(len(data) - wnd):
    temp_set = data[i:(i + wnd)].copy()
    LSTM_inputs.append(temp_set)
    for col in norm_cols:
      LSTM_inputs[i].loc[:, col] = LSTM_inputs[i].loc[:, col] / LSTM_inputs[i].loc[:, col].iloc[0] - 1  
  return LSTM_inputs

def output_for_LSTM(data,  wnd=window):
  return (data['price_close'][wnd:].values / data['price_close'][:-wnd].values) - 1

# plot the prediction of the model 
def plot_keras_model(model, show_shapes=True, show_layer_names=True):
  
  return SVG(model_to_dot(model, show_shapes=show_shapes,         show_layer_names=show_layer_names).create(prog='dot',format='svg'))
 
# Model implementation 
def build_LSTM_model(inputs, output_size, num_neurons, activ_func='tanh', dropout=0.25, loss='sparse_categorical_crossentropy', optimizer='adam'):
  
  model = Sequential() # LSTM sequence based learning
  
  # layers init of the dataset 
  model.add(LSTM(num_neurons, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2]), activation=activ_func))
  model.add(Dropout(dropout))
  model.add(LSTM(num_neurons, return_sequences=True, activation=activ_func))
  model.add(Dropout(dropout))
  model.add(LSTM(num_neurons, return_sequences=True, activation=activ_func))
  model.add(Dropout(dropout))
  model.add(LSTM(num_neurons, return_sequences=True, activation=activ_func))
  model.add(Dropout(dropout))
  model.add(LSTM(num_neurons, activation=activ_func))
  model.add(Dropout(dropout))
  
  # final output layer the size of the outout layer is 8 as we are prediction the price of the next 8 day window 
  model.add(Dense(units=output_size))
  model.add(Activation(activ_func))
  model.compile(loss=loss, optimizer=optimizer, metrics=['sparse_categorical_crossentropy'])
  
  model.summary()
  
  plot_keras_model(model, show_shapes=True, show_layer_names=False)
  return model


# get the date N days back for price prediction 
def get_date_labels():
  last_date = comb_data.iloc[0, 0]
  date_list = [last_date - datetime.timedelta(days=x) for x in range(len(X_test))]
  return[date.strftime('%m/%d/%Y') for date in date_list][::-1]


# prediction and visulize the output of the model 
def visualize_training_testing_results(date,history, model,Re_train_set,Re_X_train, Y_target,group,LatestDate):
  XTrain_real = Re_train_set['price_close'][window:].values.tolist()
  XTrain_real_skiped = Re_train_set['price_close'][:window].values.tolist()
  outputdata = ((np.transpose(model.predict(Re_X_train)) + 1) * Re_train_set['price_close'].values[:-window])
  XTrainOutput = outputdata[0]
  dataValues = []
  error = mean_absolute_percentage_error_custom(XTrain_real, XTrainOutput)
  prediction = XTrainOutput[len(XTrainOutput)-1]
  return error

window = 8

columnTOProcess = Versio1
# each group defines the features that needs to be used 
group = "pvt"
# pvt group include features , price volumn trade_count

global dailyPriceServer
LastDate = str(dailyPriceServer.iloc[len(dailyPriceServer)-1,0])

from dateutil import parser
dt = parser.parse(LastDate)
dt = dt + datetime.timedelta(days=1)
LatestDate = str(dt).split(" ")[0]
print(LatestDate)

dateIteration = LastDate
dates = []
for i in range(0,9):
  dt = parser.parse(dateIteration)
  dt = dt + datetime.timedelta(days=1)
  dateIteration = str(dt).split(" ")[0]
  dates.append(dateIteration)
  
for i in range(1,len(dates)-1):
  print(dates[i])

# check the database of the prediction of that group and data is already been generated or not 
conn = psycopg2.connect(database="lira", user = "", password = "***", host = "**", port = "**")
cur = conn.cursor()
print('SELECT *  from "predicted_price_data" WHERE date = \''+LatestDate+'\' AND \"group\" = \''+group+'\'')
cur.execute('SELECT *  from "predicted_price_data" WHERE date = \''+LatestDate+'\' AND \"group\" = \''+group+'\'')
rows = cur.fetchall()
count = 0

for row in rows:
  count = count + 1
  break;

Re_btc_data = dailyPriceServer
Re_btc_data = Re_btc_data.iloc[2:]
Re_btc_data.head()
# drop the date for pre-processing the dataset 
Re_train_set, Re_test_set = split_data_for_LSTM(Re_btc_data)
Re_train_set = Re_train_set.drop('date_volumn', 1)
Re_test_set = Re_test_set.drop('date_volumn', 1)

Re_train_set.head()
Re_test_set.head()

# Get Data for training and testing
Re_X_train = data_input_for_LSTM(Re_train_set)
Re_Y_train_btc = output_for_LSTM(Re_train_set)
Re_X_test = data_input_for_LSTM(Re_test_set)
Re_Y_test_btc = output_for_LSTM(Re_test_set)
Re_X_train, Re_X_test = to_array(Re_X_train), to_array(Re_X_test)

gc.collect()
np.random.seed(202)

# save the model for live version perdiction on the server 
model_save_name = 'prediction_price_twitter_trade.h5'

checkpoint_path = model_save_name
checkpoint_dir = os.path.dirname(checkpoint_path)
#Save weights, every 5-epochs.
cp_callback = tf.keras.callbacks.ModelCheckpoint( checkpoint_path, verbose=1, save_weights_only=True, period=5)

# initialise model architecture
btc_model = build_LSTM_model(Re_X_train, output_size=7, num_neurons=16)

# Save model weights
btc_model.save_weights(checkpoint_path.format(epoch=0))
btc_model_data = btc_model

# model training 
btc_history = btc_model_data.fit(Re_X_train, Re_Y_train_btc, epochs=20, batch_size=128, verbose=5, validation_data=(Re_X_test, Re_Y_test_btc), shuffle=False)

# model testing visualization and output of the model for live deployment 
visualize_training_testing_results(dates,btc_history, btc_model_data,Re_train_set,Re_X_train, Re_Y_train_btc, group,LatestDate)

"""('2019-04-02', ' == ', 4105.989111490249)
('2019-04-03', ' == ', 3928.5075453543664)
('2019-04-04', ' == ', 3920.645392007828)
('2019-04-05', ' == ', 3914.06008787632)
('2019-04-06', ' == ', 3914.4603049755096)
('2019-04-07', ' == ', 3911.8111811161043)
('2019-04-08', ' == ', 3929.3886876273154)
"""